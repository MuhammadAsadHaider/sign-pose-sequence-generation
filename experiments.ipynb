{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=1, circle_radius=1)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]), results.segmentation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor_image_sequences(video_path, key_points_path):\n",
    "    image_files = []\n",
    "    for filename in os.listdir(video_path):\n",
    "        if filename.endswith('.png'):\n",
    "            image_files.append(os.path.join(video_path, filename))\n",
    "\n",
    "    all_keypoints = []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, static_image_mode=False,\n",
    "    model_complexity=2) as holistic:\n",
    "        # Loop through video\n",
    "        for idx, frame in enumerate(tqdm(image_files)):\n",
    "            # Read frame\n",
    "            frame = cv2.imread(frame)\n",
    "            new_height = 300\n",
    "            # resize image\n",
    "            frame = cv2.resize(frame, (frame.shape[1], new_height))\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            # Export coordinates\n",
    "            try:\n",
    "                keypoints = extract_keypoints(results)\n",
    "                all_keypoints.append(keypoints)\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                image_path = os.path.join(video_path, f'tmp/annotated_image_{idx}.png')\n",
    "                if not cv2.imwrite(image_path , image):\n",
    "                    raise Exception(\"Could not write image\")\n",
    "            except:\n",
    "                raise Exception(f\"Could not extract keypoints for frame {idx}\")\n",
    "\n",
    "    # Convert to NumPy Array\n",
    "    all_keypoints = np.array(all_keypoints)\n",
    "\n",
    "    # save keypoints\n",
    "    if not os.path.exists(key_points_path):\n",
    "        os.makedirs(key_points_path)\n",
    "    np.save(f'{key_points_path}/kp.npy', all_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor_video(video_path, annotations_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # extract video name\n",
    "    video_name = video_path.split('/')[-1].split('.')[0]\n",
    "    # create vid folder\n",
    "    vid_folder = os.path.join(annotations_path, video_name)\n",
    "    if not os.path.exists(vid_folder):\n",
    "        os.makedirs(vid_folder)\n",
    "\n",
    "    all_keypoints = []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, static_image_mode=False,\n",
    "    model_complexity=2, enable_segmentation=True) as holistic:\n",
    "        idx = 0\n",
    "        # Loop through video\n",
    "        while cap.isOpened():\n",
    "            # Read frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            white_background = np.ones((720, 1280, 3), dtype=np.uint8) * 255\n",
    "            pose = mp.solutions.drawing_utils.landmark_pb2.NormalizedLandmarkList()\n",
    "            pose_kp = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]) if results.face_landmarks else np.zeros(468*3)\n",
    "            # save face keypoints\n",
    "            np.save(f'{vid_folder}/pkp.npy', pose_kp)\n",
    "\n",
    "            # load face keypoints\n",
    "            pose_kp = np.load(f'{vid_folder}/pkp.npy')\n",
    "\n",
    "            for pkp in pose_kp:\n",
    "                    pose.landmark.add(x=pkp[0], y=pkp[1], z=pkp[2], visibility=pkp[3])\n",
    "            mp_drawing.draw_landmarks(white_background, pose , mp_holistic.POSE_CONNECTIONS, \n",
    "                            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                            )\n",
    "            \n",
    "            cv2.imwrite('fntest.png', white_background)\n",
    "            \n",
    "            # Export coordinates\n",
    "            try:\n",
    "                keypoints, segmentation_mask = extract_keypoints(results)\n",
    "                all_keypoints.append(keypoints)\n",
    "                # save segmentation mask\n",
    "                segmentation_mask_path = os.path.join(vid_folder, f'{idx}.png')\n",
    "                # scale the pixels values to 0-255\n",
    "                segmentation_mask = (segmentation_mask * 255).astype(np.uint8)\n",
    "                if not cv2.imwrite(segmentation_mask_path , segmentation_mask):\n",
    "                    raise Exception(\"Could not write image\")\n",
    "                \n",
    "                # Draw landmarks\n",
    "                draw_landmarks(image, results)\n",
    "                # image_path = os.path.join(vid_folder, f'{idx}.png')\n",
    "                # if not cv2.imwrite(image_path , image):\n",
    "                #     raise Exception(\"Could not write image\")\n",
    "            except:\n",
    "                raise Exception(f\"Could not extract keypoints for frame {idx}\")\n",
    "            idx += 1\n",
    "\n",
    "    # Convert to NumPy Array\n",
    "    all_keypoints = np.array(all_keypoints)\n",
    "    np.save(f'{vid_folder}/kp.npy', all_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"Dataset/PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/01April_2010_Thursday_heute-6694\"\n",
    "key_points_path = \"Dataset/PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/01April_2010_Thursday_heute-6694/kp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:28<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_extractor_image_sequences(folder_path, key_points_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1134, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 311, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2062, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2098, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\UCF Masters\\Independent Study\\Code\\experiments.ipynb Cell 11\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vid_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mF:/Datasets/raw_videos/-_3bUhnn4PU_13-8-rgb_front.mp4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m annotations_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mF:/Datasets/annotations\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m feature_extractor_video(vid_path, annotations_path)\n",
      "\u001b[1;32me:\\UCF Masters\\Independent Study\\Code\\experiments.ipynb Cell 11\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(\u001b[39m'\u001b[39m\u001b[39mfntest.png\u001b[39m\u001b[39m'\u001b[39m, white_background)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Export coordinates\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     keypoints, segmentation_mask \u001b[39m=\u001b[39m extract_keypoints(results)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     all_keypoints\u001b[39m.\u001b[39mappend(keypoints)\n",
      "\u001b[1;32me:\\UCF Masters\\Independent Study\\Code\\experiments.ipynb Cell 11\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(\u001b[39m'\u001b[39m\u001b[39mfntest.png\u001b[39m\u001b[39m'\u001b[39m, white_background)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Export coordinates\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     keypoints, segmentation_mask \u001b[39m=\u001b[39m extract_keypoints(results)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UCF%20Masters/Independent%20Study/Code/experiments.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     all_keypoints\u001b[39m.\u001b[39mappend(keypoints)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1443\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:700\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1143\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1134\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2062\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2059\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2061\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[1;32m-> 2062\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2064\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2067\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2098\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2095\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2098\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   2100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2102\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vid_path = \"F:/Datasets/raw_videos/-_3bUhnn4PU_13-8-rgb_front.mp4\"\n",
    "annotations_path = \"F:/Datasets/annotations\"\n",
    "feature_extractor_video(vid_path, annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read keypoint file\n",
    "kp = np.load('kp.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294, 1662)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white background 720 x 1280\n",
    "white_background = np.ones((720, 1280, 3), dtype=np.uint8) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = mp.solutions.drawing_utils.landmark_pb2.NormalizedLandmarkList()\n",
    "face = mp.solutions.drawing_utils.landmark_pb2.NormalizedLandmarkList()\n",
    "lh = mp.solutions.drawing_utils.landmark_pb2.NormalizedLandmarkList()\n",
    "rh = mp.solutions.drawing_utils.landmark_pb2.NormalizedLandmarkList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in kp:\n",
    "    pose_kp = frame[:33*4].reshape((33, 4))\n",
    "    face_kp = frame[33*4:33*4+468*3].reshape((468, 3))\n",
    "    lh_kp = frame[33*4+468*3:33*4+468*3+21*3].reshape((21, 3))\n",
    "    rh_kp = frame[33*4+468*3+21*3:].reshape((21, 3))\n",
    "\n",
    "    for pkp in pose_kp:\n",
    "        pose.landmark.add(x=pkp[1], y=pkp[2], z=pkp[3], visibility=pkp[0])\n",
    "    for fkp in face_kp:\n",
    "        face.landmark.add(x=fkp[0], y=fkp[1], z=fkp[2])\n",
    "    for lkp in lh_kp:\n",
    "        lh.landmark.add(x=lkp[0], y=lkp[1], z=lkp[2])\n",
    "    for rkp in rh_kp:\n",
    "        rh.landmark.add(x=rkp[0], y=rkp[1], z=rkp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mp_drawing.draw_landmarks(white_background, pose, mp_holistic.POSE_CONNECTIONS) # Draw face connections\n",
    "mp_drawing.draw_landmarks(white_background, face, mp_holistic.FACEMESH_TESSELATION, \n",
    "                            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                            ) \n",
    "# mp_drawing.draw_landmarks(white_background, lh, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "# mp_drawing.draw_landmarks(white_background, rh, mp_holistic.HAND_CONNECTIONS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save image\n",
    "cv2.imwrite('test.png', white_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
